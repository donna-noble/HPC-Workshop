{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donna-noble/HPC-Workshop/blob/main/02_tokenizer_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MOsHUjgdIrIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "134e4a1b-9aa7-42a6-ded4-3b597554a8bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.7.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (5.29.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyacQSVjli1l"
      },
      "source": [
        "# What are tokenizers\n",
        "\n",
        "Before we begin, let's see a tiny little example of what are tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K4vE7CGali1m"
      },
      "outputs": [],
      "source": [
        "text = \"This is a sample sentence we use to learn about tokenizers, tokenization and tokenizing\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT0YhNzcli1n"
      },
      "source": [
        "First of all, we split the sentence into smaller components called *tokens*. This can be done in very different ways but for this example we are just using \"words\" (we are just using the space to separed said words, notice that using this strategy contracted words like *isn't* will be consider a single token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quQy-qaxli1n",
        "outputId": "d0212622-ae84-46a7-b989-177c9977333f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'a',\n",
              " 'sample',\n",
              " 'sentence',\n",
              " 'we',\n",
              " 'use',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'about',\n",
              " 'tokenizers,',\n",
              " 'tokenization',\n",
              " 'and',\n",
              " 'tokenizing']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokens = text.split(\" \")\n",
        "\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZa1mnNMli1o"
      },
      "source": [
        "Now we create a vocabulary based on the tokens we have seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2iwNTAyli1o",
        "outputId": "0185963c-9dd0-4055-f93f-c34988d6520a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'is': 1,\n",
              " 'learn': 2,\n",
              " 'we': 3,\n",
              " 'sentence': 4,\n",
              " 'tokenizing': 5,\n",
              " 'tokenizers,': 6,\n",
              " 'use': 7,\n",
              " 'tokenization': 8,\n",
              " 'to': 9,\n",
              " 'about': 10,\n",
              " 'a': 11,\n",
              " 'This': 12,\n",
              " 'sample': 13}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "unique_tokens = set(tokens) #use the set to eliminate repeted tokens\n",
        "\n",
        "vocabulary = list(unique_tokens) # just to change back the data structure to a list\n",
        "\n",
        "{vocabulary[i]:i for i in range(len(vocabulary))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWO9rzrIli1p"
      },
      "source": [
        "Finally, we encoded the sentence using the position of the token in the vocabulary list as it \"reference\" resulting in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2mgIK6bli1p",
        "outputId": "c6303352-ec68-424d-c208-5d4dd10dbbe5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12, 1, 11, 13, 4, 3, 7, 9, 2, 10, 6, 8, 0, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "encoded = []\n",
        "\n",
        "for token in tokens:\n",
        "    encoded.append(vocabulary.index(token))\n",
        "\n",
        "encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTmzMv0Qli1q"
      },
      "source": [
        "You can go back to the first cell in this notebook to edit the sentence and do the whole process again to see how different text will behave with this basic strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A normal word split works only when every word in your dataset is already known, but real text contains millions of rare forms, typos, inflections, compounds, and new words the model has never seen before. A tokenizer solves this by breaking words into subword pieces that can be reused across many words, which massively reduces vocabulary size and lets the model understand new words it has never encountered. For example, with a word split, “tokenization”, “tokenizer”, and “tokenizing” all become completely different words the model must learn from scratch. A tokenizer instead splits them into shared pieces like “token” + “ization”, “token” + “izer”, “token” + “izing”, so the model reuses knowledge and generalizes far better. This is why modern LLMs rely on tokenizers instead of raw word lists, they are more compact, handle any text, reduce memory, and allow the model to understand new words by combining familiar parts."
      ],
      "metadata": {
        "id": "Hm3sv7LioL-j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6DzXAHOli1q"
      },
      "source": [
        "# Training your own tokenizer from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs9B8-7Wli1q"
      },
      "source": [
        "In this notebook, we will see several ways to train your own tokenizer from scratch on a given corpus, so you can then use it to train a language model from scratch.\n",
        "\n",
        "Why would you need to *train* a tokenizer? That's because Transformer models very often use subword tokenization algorithms, and they need to be trained to identify the parts of words that are often present in the corpus you are using. We recommend you take a look at the [tokenization chapter](https://huggingface.co/course/chapter2/4?fw=pt) of the Hugging Face course for a general introduction on tokenizers, and at the [tokenizers summary](https://huggingface.co/transformers/tokenizer_summary.html) for a look at the differences between the subword tokenization algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOsEYlboli1q"
      },
      "source": [
        "## Getting a corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "XART_iX2olQZ",
        "outputId": "ef6e841c-3002-4eaa-976c-6786d90599ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "e7116a75675448c28a1b0f4f85fe95f2",
            "ecd59d3d03f74c58b2eb4b4d448464ea",
            "cfea05715f9d4ac986af6aae8ecb5e15",
            "e4eb5e0a51d9472da1dc7bf842520117",
            "7df762fa46c34fad8c9ba195d03323fd",
            "702c123791614a56a99fa9375bcf16ee",
            "b47fb3f8227742619c793da62e502513",
            "cbc2ae5a380f4d0aa8abee5b80217902",
            "8ac42fe7b56d49ea8c7ccdea7a428993",
            "c3cec5fbacd64c92aab8922da8c5fd03",
            "6c6ef0d4ae6b406695c1b06e4b0d5b71",
            "b1f1ce921e4549a1951472bdfd2b2578",
            "1c7df5ebf62b4102801338e51640d3cc",
            "e1ccf65231b94920a4706025c4c26d70",
            "5b67048f18d24bd58af0d9f251ed3ed5",
            "93795ea89d694d3498416e3d8b53da1a",
            "23daa8db021c41baae5eef451688d3dd",
            "264de8f6f1ae470eaeff0491c1a711e8",
            "35a7301150fa4ae5b486ec312c04e1b5",
            "480c066d03404a81a0c25584fc7560f6"
          ]
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.11.12)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7116a75675448c28a1b0f4f85fe95f2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load in streaming mode — does NOT download whole dataset\n",
        "ds = load_dataset(\"LSX-UniWue/LLaMmlein-Dataset\", streaming=True)\n",
        "\n",
        "stream = ds[\"train\"].shuffle(seed=42, buffer_size=100_000)\n",
        "\n",
        "# Take first 10k from the shuffled stream\n",
        "sampled = []\n",
        "for i, item in enumerate(stream):\n",
        "    if i >= 10_000:\n",
        "        break\n",
        "    sampled.append(item)\n",
        "\n",
        "# Convert list → Dataset object (optional)\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_list(sampled)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "5bef17b43ac7418098b90dae552a6eb3",
            "778b2caa0e144f41ac6078ff7149bbc6",
            "bd2eab50f1a840ae954f651053a6f3a5",
            "3059a5657cbc4b9d8bd43cc0730ae978",
            "62e8b63a83e047c2bacdde4473a3c670",
            "466c745ae832424fb6f9de0b1817a052",
            "51699cc19d874019833b7a2f9d3ecce7",
            "a53f7626b08f47529e40a8f8c69d7514",
            "2374cb0a1f554a96acabafe43f131e02",
            "407968a725374b8db07e69958c4fc897",
            "ccdf5a08113a4e6eb2ae25ec77f5d823"
          ]
        },
        "id": "fJ67Mwtmmq8e",
        "outputId": "081b54ad-a9bf-416b-aa4e-a8491b2370b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/1811 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bef17b43ac7418098b90dae552a6eb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'text', 'source'],\n",
            "    num_rows: 10000\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU2cXUeYli1r"
      },
      "source": [
        "We can have a look at the dataset, which as 36,718 texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ1jyK45li1r",
        "outputId": "de9c7021-856c-45eb-b64a-8fd92cfe43d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'text', 'source'],\n",
              "    num_rows: 10000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXmUoYN8li1r"
      },
      "source": [
        "To access an element, we just have to provide its index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21_SM5_Vli1r",
        "outputId": "34a37bf6-f005-43de-8636-840998af10ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'sha1:YYEBPYNDQOOGJ6NTXIO3ERIZEDFJL6M5',\n",
              " 'text': '2626. September 2022 1010. Oktober 2022',\n",
              " 'source': 'https://www.classic-computing.org/veranstaltungskalender/?time=day&yr=2022&month=10&dy=2'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2B17Ni4li1r"
      },
      "source": [
        "We can also access a slice directly, in which case we get a dictionary with the key `\"text\"` and a list of texts as value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClmDQySRli1r",
        "outputId": "1641467b-b7f0-4aaa-b7b8-0dfca13acca6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': ['sha1:J3LZP6K4YICBH4LVXAKEVZCCOGADP73Q',\n",
              "  'sha1:YYEBPYNDQOOGJ6NTXIO3ERIZEDFJL6M5'],\n",
              " 'text': ['Initiative hatte für Kreuzberger Gemüseladen gekämpft – „Bizim Bakkal“ schließt aus gesundheitlichen Gründen\\n„Nun ist doch Schluss mit dem Gemüseladen „Bizim Bakkal“. Ahmet Caliskan gibt das Geschäft aus gesundheitlichen Gründen auf. Seine Nachbarschaft hatte lange für den Laden gekämpft – im vergangenen Sommer kam es zu heftigen Protesten gegen die Kündigung seitens des Vermieters. Für Caliskan hatte das langfristige Folgen.\\nAhmet Caliskan gibt auf. Der Inhaber des Kreuzberger Gemüseladens „Bizim Bakkal“ schließt sein Geschäft im Wrangelkiez aus gesundheitlichen Gründen, wie die Nachbarschaftsinitiative „Bizim Kiez“ am Montag bekannt gab. Im vergangenen Sommer hatte Caliskan die Kündigung seines Mietvertrages erhalten. Daraufhin formierte sich „Bizim Kiez“ („Wir sind der Kiez“) und kämpfte erfolgreich über Monate dafür, dass der kleine Laden erhalten bleibt.\\nCaliskan bekam seinen alten Mietvertrag zurück. Der ist jedoch unbefristet und kann daher mit einem halben Jahr Frist jederzeit gekündigt werden. „Unter diesem enormen psychischen Druck und ohne planbare Perspektive kann die Familie den Betrieb nicht mehr aufrechterhalten“, schreibt die Initiative. Seine gesundheitlichen Probleme seien dem Vermieter bekannt gewesen. „Bizim Bakkal“ schließt zum 31. März.“\\n„Nach Gewerbemietrecht weiter kündbar – Zukunft des Gemüseladens Bizim Bakkal noch ungewiss ← Tagesspiegel: Bizim Bakkal schließt wohl doch Die Welt: Gemüseladen „Bizim Bakkal“ schließt nun doch →',\n",
              "  '2626. September 2022 1010. Oktober 2022'],\n",
              " 'source': ['https://www.bizim-kiez.de/blog/2016/02/01/rbb-bizim-bakkal-schliesst-aus-gesundheitlichen-gruenden/',\n",
              "  'https://www.classic-computing.org/veranstaltungskalender/?time=day&yr=2022&month=10&dy=2']}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dataset[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qemi6H6Xli1r"
      },
      "source": [
        "The API to train our tokenizer will require an iterator of batch of texts, for instance a list of list of texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MCQ-MYMIli1s"
      },
      "outputs": [],
      "source": [
        "batch_size = 1000\n",
        "\n",
        "all_texts = []\n",
        "\n",
        "for i in range(0, len(dataset),batch_size):\n",
        "    all_texts.append(dataset[i : i + batch_size][\"text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfuWTdGsli1s"
      },
      "source": [
        "To avoid loading everything into memory (since the Datasets library keeps the element on disk and only load them in memory when requested), we define a Python iterator. This is particularly useful if you have a huge dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d5vBGoHpli1s"
      },
      "outputs": [],
      "source": [
        "def batch_iterator():\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        yield dataset[i : i + batch_size][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adapting an Existing Tokenizer vs Training a Tokenizer From Scratch\n",
        "\n",
        "- Training a tokenizer from scratch gives much better compression and fairness for under-represented languages, because the vocabulary is built around your data instead of mostly English.\n",
        "\n",
        "-\tExtending an existing tokenizer preserves compatibility with pretrained models, making adaptation far cheaper and faster without retraining embeddings from scratch."
      ],
      "metadata": {
        "id": "v1Qt78IXq6cf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCTp8QBli1y"
      },
      "source": [
        "# **Building your tokenizer from scratch**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs3ZabS_li1z"
      },
      "source": [
        "To understand how to build your tokenizer from scratch, we have to dive a little bit more in the   Tokenizers library and the tokenization pipeline. This pipeline takes several steps:\n",
        "\n",
        "- **Normalization**: Executes all the initial transformations over the initial input string. For example when you need to lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer.\n",
        "- **Pre-tokenization**: In charge of splitting the initial input string. That's the component that decides where and how to pre-segment the origin string. The simplest example would be to simply split on spaces.\n",
        "- **Model**: Handles all the sub-token discovery and generation, this is the part that is trainable and really dependent of your input data.\n",
        "- **Post-Processing**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n",
        "\n",
        "And to go in the other direction:\n",
        "\n",
        "- **Decoding**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according to the `PreTokenizer` we used previously.\n",
        "\n",
        "For the training of the model, the  Tokenizers library provides a `Trainer` class that we will use.\n",
        "\n",
        "All of these building blocks can be combined to create working tokenization pipelines. To give you some examples, we will show three full pipelines here: how to replicate GPT-2, BERT and T5 (which will give you an example of Byte Pair Encoding, WordPiece and Unigram tokenizer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX-QGLlFli1z"
      },
      "source": [
        "### **Subword tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UboRMU5li1z"
      },
      "source": [
        "In this group of cells, we will include a more detailed explanation of subword tokenization. Byte Pair Encoding (BPE), WordPiece, and Unigram are all different kinds of what is known as subword tokenization.\n",
        "\n",
        "Subword tokenization algorithms are based on the idea of considering units smaller than words as tokens. In particular, uncommon words in the vocabulary will be split into smaller units following different approaches. For example, the word *darkest* will be divided into *dark* and *est*, adding both these tokens to the vocabulary, but not the original word. We can already see how these approaches can be advantageous.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **BPE** – merges the most frequent character pairs first, gradually building bigger subword units based on frequency.\n",
        "\n",
        "\n",
        "\n",
        "- **WordPiece** – selects subword units that maximize the probability of reconstructing the training text using a language-model–based criterion.\n",
        "\n",
        "\n",
        "\n",
        "- **Unigram** – starts with a large vocabulary and removes subwords that least reduce the likelihood, keeping only the most useful ones."
      ],
      "metadata": {
        "id": "pgmdwdLyuc8D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3Lq1Iqkli1z"
      },
      "source": [
        "# **WordPiece model like BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YbI9Mhwli1z"
      },
      "source": [
        "Let's have a look at how we can create a WordPiece tokenizer like the one used for training BERT. The first step is to create a `Tokenizer` with an empty `WordPiece` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-lsQyxl_li1z"
      },
      "outputs": [],
      "source": [
        "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(models.WordPiece(unl_token=\"[UNK]\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JXb8Eotli1z"
      },
      "source": [
        "This `tokenizer` is not ready for training yet. We have to add some preprocessing steps: the normalization (which is optional) and the pre-tokenizer, which will split inputs into the chunks we will call words. The tokens will then be part of those words (but can't be larger than that).\n",
        "\n",
        "In the case of BERT, the normalization is lowercasing. Since BERT is such a popular model, it has its own normalizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xa7SqpwNli1z"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65RqV181li1z"
      },
      "source": [
        "If you want to customize it, you can use the existing blocks and compose them in a sequence: here for instance we lower case, apply NFD normalization (unicode normalization) and strip the accents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d6-v6vxgli1z"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E10AB5BYli1z"
      },
      "source": [
        "There is also a `BertPreTokenizer` we can use directly. It pre-tokenizes using white space and punctuation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pNGcUG5yli1z"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrZ6PhV2li10"
      },
      "source": [
        "Like for the normalizer, we can combine several pre-tokenizers in a `Sequence`. If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C6HTM9mli10"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Initiative hatte für Kreuzberger Gemüseladen gekämpft – „Bizim Bakkal“\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld3HJBesli10"
      },
      "source": [
        "Note that the pre-tokenizer not only split the text into words but keeps the offsets, that is the beginning and start of each of those words inside the original text. This is what will allow the final tokenizer to be able to match each token to the part of the text that it comes from (a feature we use for question answering or token classification tasks).\n",
        "\n",
        "We can now train our tokenizer (the pipeline is not entirely finished but we will need a trained tokenizer to build the post-processor), we use a `WordPieceTrainer` for that. The key thing to remember is to pass along the special tokens to the trainer, as they won't be seen in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CDcF4eyli10"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciJUngANli10"
      },
      "source": [
        "To actually train the tokenizer, the method looks like what we used before: we can either pass some text files, or an iterator of batches of texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV-0V238li10"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbyYjLWmli10"
      },
      "source": [
        "Now that the tokenizer is trained, we can define the post-processor: we need to add the CLS token at the beginning and the SEP token at the end (for single sentences) or several SEP tokens (for pairs of sentences). We use a [`TemplateProcessing`](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.processors.TemplateProcessing) to do this, which requires to know the IDs of the CLS and SEP token (which is why we waited for the training).\n",
        "\n",
        "So let's first grab the ids of the two special tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rS6TqWTli10"
      },
      "outputs": [],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "print(cls_token_id, sep_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQjEPxIOli10"
      },
      "source": [
        "And here is how we can build our post processor. We have to indicate in the template how to organize the special tokens with one sentence (`$A`) or two sentences (`$A` and `$B`). The `:` followed by a number indicates the token type ID to give to each part, we use a binary token type ID to indicate, in case that there is, what tokens belong to the first sentence and what tokens belong to the second sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXHntWupli10"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", cls_token_id),\n",
        "        (\"[SEP]\", sep_token_id),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lqZh0Nili10"
      },
      "source": [
        "We can check we get the expected results by encoding a pair of sentences for instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-kmFBtYli10"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer.encode(\"Für Caliskan hatte das langfristige Folgen.\",\"\\nAhmet Caliskan gibt auf.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-SNe4Htli10"
      },
      "source": [
        "We can look at the tokens to check the special tokens have been inserted in the right places:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6TPCBe3li11"
      },
      "outputs": [],
      "source": [
        "encoding.tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE8aSeGTli11"
      },
      "source": [
        "And we can check the token type ids are correct (the tokens that belong to the first sentence, including the special tokens [CLS] and [SEP] should get a 0 and the tokens that belong to the second sentence should get a 1):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV8ivyjEli11"
      },
      "outputs": [],
      "source": [
        "encoding.type_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKvq2rpfli11"
      },
      "source": [
        "The last piece in this tokenizer is the decoder, we use a `WordPiece` decoder and indicate the special prefix `##`, used by our tokenizer to indicate tokens that were subparts of a word (for example, the word *sentence* in the last example was divided as *sent* and *##ence*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67mmgI5Sli11"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLsjRS9Ili11"
      },
      "source": [
        "Now that our tokenizer is finished, we need to wrap it inside a Transformers object to be able to use it with the Transformers library. More specifically, we have to put it inside the class of tokenizer fast corresponding to the model we want to use, here a `BertTokenizerFast`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W2uvGNKli12"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x08QIK9li12"
      },
      "source": [
        "And like before, we can use this tokenizer as a normal Transformers tokenizer, and use the `save_pretrained` or `push_to_hub` methods.\n",
        "\n",
        "If the tokenizer you are building does not match any class in Transformers because it's really special, you can wrap it in `PreTrainedTokenizerFast`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_QgN9Ayli12"
      },
      "source": [
        "# **Byte Pair Encoding model like GPT-2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEvZFjxtli12"
      },
      "source": [
        "Let's now have a look at how we can create a Byte Pair Encoding (BPE) tokenizer like the one used for training GPT-2. The first step is to create a `Tokenizer` with an empty `BPE` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZdiJ1fBli12"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMODHvO5li12"
      },
      "source": [
        "Like before, we have to add the optional normalization (not used in the case of GPT-2) and we need to specify a pre-tokenizer before training. In the case of GPT-2, the pre-tokenizer used is a byte level pre-tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_CHWT_vli12"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B46iboOtli12"
      },
      "source": [
        "If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VldNQk4fli12"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str( \"Für Caliskan hatte das langfristige Folgen.\\nAhmet Caliskan gibt auf.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLlloMozli12"
      },
      "source": [
        "We used the same default as for GPT-2 for the prefix space, so you can see that each word gets an initial `'Ġ'` added at the beginning, except the first one.\n",
        "\n",
        "We can now train our tokenizer! This time we use a `BpeTrainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KrplBohli12"
      },
      "outputs": [],
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fi03is2li12"
      },
      "source": [
        "To finish the whole pipeline, we have to include the post-processor and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP0cp6RMli12"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
        "tokenizer.decoder = decoders.ByteLevel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCr1VN4Mli12"
      },
      "source": [
        "And like before, we finish by wrapping this in a Transformers tokenizer object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Maq-ULLli12"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode( \"'Initiative hatte für Kreuzberger Gemüseladen gekämpft – „Bizim Bakkal“ schließt aus gesundheitlichen Gründen\\n„Nun ist doch Schluss mit dem Gemüseladen „Bizim Bakkal“. Ahmet Caliskan gibt das Geschäft aus gesundheitlichen Gründen auf. Seine Nachbarschaft hatte lange für den Laden gekämpft – im vergangenen Sommer kam es zu heftigen Protesten gegen die Kündigung seitens des Vermieters. Für Caliskan hatte das langfristige Folgen.\\nAhmet Caliskan gibt auf. Der Inhaber des Kreuzberger Gemüseladens „Bizim Bakkal“ schließt sein Geschäft im Wrangelkiez aus gesundheitlichen Gründen, wie die Nachbarschaftsinitiative „Bizim Kiez“ am Montag bekannt gab. Im vergangenen Sommer hatte Caliskan die Kündigung seines Mietvertrages erhalten.\").tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPedQcO6li13"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "new_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzeKeMxdli13"
      },
      "source": [
        "# **Unigram model like Albert,T5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ5ea6OXli13"
      },
      "source": [
        "Let's now have a look at how we can create a Unigram tokenizer like the one used for training T5. The first step is to create a `Tokenizer` with an empty `Unigram` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUIpekRCli13"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.Unigram())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvqwdM2Mli13"
      },
      "source": [
        "Like before, we have to add the optional normalization (here some replaces and lower-casing) and we need to specify a pre-tokenizer before training. The pre-tokenizer used is a `Metaspace` pre-tokenizer: it replaces all spaces by a special character (defaulting to ▁) and then splits on that character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI0NGQtnli13"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.Replace(\"``\", '\"'), normalizers.Replace(\"''\", '\"'), normalizers.Lowercase()]\n",
        ")\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkmIMUd8li13"
      },
      "source": [
        "If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_0kl9R_li13"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"'Initiative hatte für Kreuzberger Gemüseladen gekämpft – „Bizim Bakkal“ schließt aus gesundheitlichen Gründen\\n„Nun ist doch Schluss mit dem Gemüseladen „Bizim Bakkal“. Ahmet Caliskan gibt das Geschäft aus gesundheitlichen Gründen auf. Seine Nachbarschaft hatte lange für den Laden gekämpft – im vergangenen Sommer kam es zu heftigen Protesten gegen die Kündigung seitens des Vermieters. Für Caliskan hatte das langfristige Folgen.\\nAhmet Caliskan gibt auf. Der Inhaber des Kreuzberger Gemüseladens „Bizim Bakkal“ schließt sein Geschäft im Wrangelkiez aus gesundheitlichen Gründen, wie die Nachbarschaftsinitiative „Bizim Kiez“ am Montag bekannt gab. Im vergangenen Sommer hatte Caliskan die Kündigung seines Mietvertrages erhalten.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4-8Zp_sli13"
      },
      "source": [
        "You can see that each word gets an initial `▁` added at the beginning, as is usually done by sentencepiece.\n",
        "\n",
        "We can now train our tokenizer! This time we use a `UnigramTrainer`.\"We have to explicitely set the unknown token in this trainer otherwise it will forget it afterward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0Tv8h4Dli13"
      },
      "outputs": [],
      "source": [
        "trainer = trainers.UnigramTrainer(vocab_size=25000, special_tokens=[\"[CLS]\", \"[SEP]\", \"<unk>\", \"<pad>\", \"[MASK]\"], unk_token=\"<unk>\")\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55tT5Mr5li13"
      },
      "source": [
        "To finish the whole pipeline, we have to include the post-processor and decoder. The post-processor is very similar to what we saw with BERT, the decoder is just `Metaspace`, like for the pre-tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q80qHjgpli13"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"'Initiative hatte für Kreuzberger Gemüseladen gekämpft – „Bizim Bakkal“ schließt aus gesundheitlichen Gründen\\n„Nun ist doch Schluss mit dem Gemüseladen „Bizim Bakkal“. Ahmet Caliskan gibt das Geschäft aus gesundheitlichen Gründen auf. Seine Nachbarschaft hatte lange für den Laden gekämpft – im vergangenen Sommer kam es zu heftigen Protesten gegen die Kündigung seitens des Vermieters. Für Caliskan hatte das langfristige Folgen.\\nAhmet Caliskan gibt auf. Der Inhaber des Kreuzberger Gemüseladens „Bizim Bakkal“ schließt sein Geschäft im Wrangelkiez aus gesundheitlichen Gründen, wie die Nachbarschaftsinitiative „Bizim Kiez“ am Montag bekannt gab. Im vergangenen Sommer hatte Caliskan die Kündigung seines Mietvertrages erhalten.\").tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbanJfOFli13"
      },
      "outputs": [],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFm3yE8zli13"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", cls_token_id),\n",
        "        (\"[SEP]\", sep_token_id),\n",
        "    ],\n",
        ")\n",
        "tokenizer.decoder = decoders.Metaspace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4atNEft-li13"
      },
      "source": [
        "And like before, we finish by wrapping this in a Transformers tokenizer object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A716BS5vli13"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertTokenizerFast\n",
        "\n",
        "new_tokenizer = AlbertTokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hmV6dRsdqSDL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e7116a75675448c28a1b0f4f85fe95f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_b47fb3f8227742619c793da62e502513"
          }
        },
        "ecd59d3d03f74c58b2eb4b4d448464ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbc2ae5a380f4d0aa8abee5b80217902",
            "placeholder": "​",
            "style": "IPY_MODEL_8ac42fe7b56d49ea8c7ccdea7a428993",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "cfea05715f9d4ac986af6aae8ecb5e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c3cec5fbacd64c92aab8922da8c5fd03",
            "placeholder": "​",
            "style": "IPY_MODEL_6c6ef0d4ae6b406695c1b06e4b0d5b71",
            "value": ""
          }
        },
        "e4eb5e0a51d9472da1dc7bf842520117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_b1f1ce921e4549a1951472bdfd2b2578",
            "style": "IPY_MODEL_1c7df5ebf62b4102801338e51640d3cc",
            "value": true
          }
        },
        "7df762fa46c34fad8c9ba195d03323fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e1ccf65231b94920a4706025c4c26d70",
            "style": "IPY_MODEL_5b67048f18d24bd58af0d9f251ed3ed5",
            "tooltip": ""
          }
        },
        "702c123791614a56a99fa9375bcf16ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93795ea89d694d3498416e3d8b53da1a",
            "placeholder": "​",
            "style": "IPY_MODEL_23daa8db021c41baae5eef451688d3dd",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b47fb3f8227742619c793da62e502513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "cbc2ae5a380f4d0aa8abee5b80217902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ac42fe7b56d49ea8c7ccdea7a428993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3cec5fbacd64c92aab8922da8c5fd03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c6ef0d4ae6b406695c1b06e4b0d5b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1f1ce921e4549a1951472bdfd2b2578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c7df5ebf62b4102801338e51640d3cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1ccf65231b94920a4706025c4c26d70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b67048f18d24bd58af0d9f251ed3ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "93795ea89d694d3498416e3d8b53da1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23daa8db021c41baae5eef451688d3dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "264de8f6f1ae470eaeff0491c1a711e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35a7301150fa4ae5b486ec312c04e1b5",
            "placeholder": "​",
            "style": "IPY_MODEL_480c066d03404a81a0c25584fc7560f6",
            "value": "Connecting..."
          }
        },
        "35a7301150fa4ae5b486ec312c04e1b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "480c066d03404a81a0c25584fc7560f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bef17b43ac7418098b90dae552a6eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_778b2caa0e144f41ac6078ff7149bbc6",
              "IPY_MODEL_bd2eab50f1a840ae954f651053a6f3a5",
              "IPY_MODEL_3059a5657cbc4b9d8bd43cc0730ae978"
            ],
            "layout": "IPY_MODEL_62e8b63a83e047c2bacdde4473a3c670"
          }
        },
        "778b2caa0e144f41ac6078ff7149bbc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_466c745ae832424fb6f9de0b1817a052",
            "placeholder": "​",
            "style": "IPY_MODEL_51699cc19d874019833b7a2f9d3ecce7",
            "value": "Resolving data files: 100%"
          }
        },
        "bd2eab50f1a840ae954f651053a6f3a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a53f7626b08f47529e40a8f8c69d7514",
            "max": 1811,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2374cb0a1f554a96acabafe43f131e02",
            "value": 1811
          }
        },
        "3059a5657cbc4b9d8bd43cc0730ae978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_407968a725374b8db07e69958c4fc897",
            "placeholder": "​",
            "style": "IPY_MODEL_ccdf5a08113a4e6eb2ae25ec77f5d823",
            "value": " 1811/1811 [00:00&lt;00:00, 173.49it/s]"
          }
        },
        "62e8b63a83e047c2bacdde4473a3c670": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "466c745ae832424fb6f9de0b1817a052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51699cc19d874019833b7a2f9d3ecce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a53f7626b08f47529e40a8f8c69d7514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2374cb0a1f554a96acabafe43f131e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "407968a725374b8db07e69958c4fc897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccdf5a08113a4e6eb2ae25ec77f5d823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}